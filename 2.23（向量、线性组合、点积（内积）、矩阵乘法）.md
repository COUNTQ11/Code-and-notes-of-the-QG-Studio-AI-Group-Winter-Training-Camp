# 机器学习数学基础学习笔记

**日期**：2026-02-23
**学习主题**：向量、线性组合、点积（内积）、矩阵乘法

------

## 一、向量（Vector）

### 1. **定义**

- 向量是**有序的数值列表**，表示方向和大小（在机器学习中，常用于表示特征或数据点）。
- **维度**：n维向量有n个元素，通常用列向量表示。
- **符号**：用粗体小写字母表示（如 **v**,**w** )。

### 2. **关键运算**

| 运算         | 公式                                    | 例子                   |
| :----------- | :-------------------------------------- | :--------------------- |
| **向量加法** | **v**+**w**=[*v*1+*w*1 *v*2+*w*2]       | [12]+[34]=[46]         |
| **数乘**     | *c*⋅**v**=[*c*⋅*v*1*c*⋅*v*2] (c 为标量) | 2⋅[12]=[24]2⋅[12]=[24] |

### 3. **为什么重要？**

- 在机器学习中，**每个数据点**可表示为一个向量。
- **线性模型**（如线性回归）的核心操作就是向量运算。

------

## 二、线性组合（Linear Combination）

### 1. **定义**

- 用**标量（数字）** 乘以多个向量后相加，得到新向量：
- **u**=*c*1**v**1+*c*2**v**2+⋯+*c*k***v**k*
  其中 c1,c2,…,c*k* 是标量，v1,…,vk 是向量。

### 2. **例子**

- 设 v=[10],w=[01] （单位向量）
- 线性组合： 3**v**+2**w**=3[10]+2[01]=[32]
- **几何意义**：在2D空间中，线性组合表示从原点出发，沿着 v 和 w 方向移动后到达的点。

### 3. **为什么重要？**

- **机器学习核心**：模型参数（如权重）本质上是线性组合的系数。
  例如，线性回归： y*=*w*1​*x*1​+*w*2​*x*2​ 就是向量 [x1​,x2​] 与权重向量 [w1​,w2​] 的线性组合。
- **线性代数基础**：理解“向量空间”和“基”的概念。

------

## 三、点积（内积，Dot Product）

### 1. **定义**

- 两个**同维度向量**的对应元素相乘后求和，结果是**标量**：
  **v**⋅**w**=*v*1​*w*1​+*v*2​*w*2​+⋯+*v**n*​*w**n*​
- **符号**：也写作⟨**v**,**w**⟩ 或v*^**T***w(转置后矩阵乘法）。

### 2. **公式与几何意义**

表格

| 公式                                                        | 几何意义                                                     |
| :---------------------------------------------------------- | :----------------------------------------------------------- |
| **v**⋅**w**=∣**v**∣∣**w**∣cos*θ*                            | *θ* 是两向量夹角， ∣⋅∣∣⋅∣ 是向量长度（模）                   |
| **计算示例**： v**=[12],**w**=[34] **v**⋅**w=1×3+2×4=3+8=11 | - 若点积=0 → 两向量**正交**（垂直） - 若点积>0 → 夹角<90° - 若点积<0 → 夹角>90° |

### 3. **为什么重要？**

- 机器学习应用
  - **相似度计算**：点积衡量向量相似性（如推荐系统中计算用户兴趣相似度）。
  - **损失函数**：在逻辑回归、神经网络中，点积用于计算加权输入（如 *z*=**w**^T**x**）。
  - **投影**：点积用于计算一个向量在另一个向量上的投影长度。

------

## 四、矩阵乘法（Matrix Multiplication）

### 1. **定义**

- 矩阵A（m×n）与矩阵B（n×p）相乘，结果C（m×p）的每个元素是A的**行**与B的**列**的**点积**：
  *C**ij**=rowi*(*A*)⋅colj*(*B)
- **关键规则**：A的**列数**必须等于B的**行数**（否则无法相乘）。

### 2. **为什么重要？**

- 机器学习核心操作:
  - **神经网络**：输入层到隐藏层的计算是矩阵乘法（如z**=**Wx** ，其中 **W** 是权重矩阵）。
  - **数据变换**：主成分分析（PCA）中，数据矩阵与特征向量矩阵相乘。
  - **效率**：矩阵乘法是深度学习框架（如TensorFlow/PyTorch）的底层计算。

------

## 五、关键总结与易错点

表格

| 概念         | 核心要点                                      | 常见错误                                       |
| :----------- | :-------------------------------------------- | :--------------------------------------------- |
| **向量**     | 有序列表，维度固定；加法/数乘逐元素操作       | 混淆行向量 vs 列向量（机器学习中多用列向量）   |
| **线性组合** | 标量 × 向量 → 求和；是线性模型的基础          | 误以为必须用单位向量（实际可任意向量）         |
| **点积**     | 同维度向量 → 标量；几何意义是投影和夹角       | 误用于不同维度向量（必须维度相同）             |
| **矩阵乘法** | A(m×n) × B(n×p) → C(m×p)；**列数=行数**是前提 | 忘记检查维度（如 2×3 矩阵 × 4×2 矩阵无法相乘） |

------

## 六、为什么这些是机器学习的基础？

- **向量**：数据的表示形式（特征向量）。
- **线性组合**：模型的输入输出关系（如 y=*w*1*x*1+*w*2*x*2 ）。
- **点积**：计算相似度、权重贡献（如神经网络激活前的加权和）。
- **矩阵乘法**：高效处理多维数据（如批量数据的批量计算）。